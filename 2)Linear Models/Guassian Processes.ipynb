{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Ready\n",
    "\n",
    "In this chapter we'll use Guassian Processes for regression.  In the linear models section we saw how representing prior information on the coefficients was possible using Bayesian Ridge Regression.  Here we'll do something similar.  Instead of putting a prior on the coefficients we're going to put a prior on the functional form of the system and we're going to assume that prior is Gaussian.\n",
    "\n",
    "Generally with a Guassian Process we assume the mean is 0, so it's the covariance function we'll need to specify.\n",
    "\n",
    "So let's use some regression data and walk through how Gaussian Processes work on sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-e52fc3a23c97>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mboston_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mboston\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mtrain_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mboston_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m.75\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m.25\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "\n",
    "boston_X = boston.data\n",
    "boston_y = boston.target\n",
    "\n",
    "train_set = np.random.choice([True, False], len(boston_y), p=[.75, .25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to do it\n",
    "\n",
    "How that we have the data we'll create an sklearn GuassianProcess object.  By default it uses a constant regression function and squared exponential correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcess\n",
    "\n",
    "gp = GaussianProcess()\n",
    "\n",
    "gp.fit(boston_X[train_set], boston_y[train_set])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a formatible object definition.  A couple things to point out.\n",
    "\n",
    "* `beta0` the regression weights, this defaults to such that MLE is used for estimation\n",
    "* `corr`: this is the correlation function.  There are several built in correlation function.  We'll look at more of them in the \"How it works\" section\n",
    "* `regr`: this is the constant regression function.\n",
    "* `nugget`: This is the regularization parameter.  It defaults to a very small number.  You can either pass one value to be used for each data point or a single value to applied uniformly.\n",
    "\n",
    "Ok so now that we've fit the object let's look at it's performance against the test object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = gp.predict(boston_X[~train_set])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the predicted values verse the actual values, then because we're doing regression it's probably a good idea to look at a plotted residuals and histogram of the residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(10, 7), nrows=3)\n",
    "\n",
    "f.tight_layout()\n",
    "\n",
    "ax[0].plot(range(len(test_preds)), test_preds, label='Predicted Values');\n",
    "ax[0].plot(range(len(test_preds)), boston_y[~train_set], label='Actual Values');\n",
    "ax[0].set_title(\"Predicted vs Actuals\")\n",
    "ax[0].legend(loc='best')\n",
    "\n",
    "ax[1].plot(range(len(test_preds)), test_preds - boston_y[~train_set]);\n",
    "ax[1].set_title(\"Plotted Residuals\")\n",
    "\n",
    "ax[2].hist(test_preds - boston_y[~train_set]);\n",
    "ax[2].set_title(\"Histogram of Residuals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How it works\n",
    "\n",
    "Now that we've worked through a very quick example, let's look a little more about what some of the parameters are doing and how we can tune them based on the model we're tryin to fit.\n",
    "\n",
    "First let's try to understand what's going on with the `corr` function.  This function describes the relationship between the different pairs of X.  Scikit-Learn offers 5 different correlation functions:\n",
    "\n",
    "* absolute_exponential\n",
    "* squared_exponential\n",
    "* generalized_exponential\n",
    "* cubic\n",
    "* linear\n",
    "\n",
    "For example, the squared expoential has the form:\n",
    "$$\n",
    "K = \\exp(-{\\frac{|d|^2}{2l^2}})\n",
    "$$\n",
    "\n",
    "Linear on the other hand is just dot product of the two points in question.\n",
    "\n",
    "$$\n",
    "K = x^T x^{'}\n",
    "$$\n",
    "\n",
    "Another parameter of interest is `theta0`.  This represents the starting point in the estimation of the of the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have an estiamtion of K and the mean.  Our process if fully specied due to it being a Guassian Processes.  Therefore, we can now get the posterior predictive distribution which is used for prediction given unseen Xs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a different `regr` function and apply a different `theta0` and look at how the predictions differ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp = GaussianProcess(regr='linear', theta0=5e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp.fit(boston_X[train_set], boston_y[train_set]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_preds = gp.predict(boston_X[~train_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(7, 5))\n",
    "\n",
    "f.tight_layout()\n",
    "\n",
    "ax.hist(test_preds - boston_y[~train_set], label='Residuals Original', color='b', alpha=.5);\n",
    "ax.hist(linear_preds - boston_y[~train_set], label='Residuals Linear', color='r', alpha=.5);\n",
    "ax.set_title(\"Residuals\")\n",
    "ax.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly the second model's predictions are better for the most part.  If we wanted to sum this up we could look at the MSE of the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.power(test_preds - boston_y[~train_set], 2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.power(linear_preds - boston_y[~train_set], 2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# There's More"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might want to understand the uncertainty in our estimates.  When we're making the predictions if we pass the argument `eval_MSE` as `True` we'll get the MSE as well as the predicted values.  This is done as a tuple during the return process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds, MSE = gp.predict(boston_X[~train_set], eval_MSE=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now that we have errors in the estimates let's plot a the first few to get an indication of inaccuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(7, 5))\n",
    "\n",
    "n = 20\n",
    "rng = range(n)\n",
    "ax.scatter(rng, test_preds[:n])\n",
    "ax.errorbar(rng, test_preds[:n], yerr=1.96*MSE[:n])\n",
    "\n",
    "ax.set_title(\"Predictions with Error Bars\")\n",
    "\n",
    "ax.set_xlim((-1, 21));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see there's quite a bit of variance in the estimates for a lot of these points.  On the other hand our overall error wasn't too bad."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
